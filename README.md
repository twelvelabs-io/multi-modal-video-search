# Multi-Vector Video Search Pipeline

A video semantic search pipeline built with AWS Bedrock Marengo 3.0, featuring dual vector storage backends: **MongoDB Atlas** and **Amazon S3 Vectors**.

---

## ğŸ¯ Multi-Vector Search Architecture

This system uses a **multi-vector retrieval architecture** where each video segment stores **three separate embedding vectors** (visual, audio, transcription) instead of one combined embedding.

**Why separate embeddings?** This allows you to adjust the importance of each modality at search time without re-indexing. You can search the same data with different strategies depending on your query.

**Storage Architecture:**
```
Video Segment â†’ Three 512d Embeddings:
  â”œâ”€ Visual Embedding      (visual content, scenes, actions)
  â”œâ”€ Audio Embedding       (sounds, music, ambient audio)
  â””â”€ Transcription Embedding (spoken words, dialogue)
```

**Advantages:**
- âœ… Preserves modality-specific signal fidelity
- âœ… Transparent, modality-level debuggability
- âœ… Change weights without re-indexing
- âœ… Supports modality-specific optimization
- âœ… Foundation for adaptive architectures

**Drawbacks:**
- âŒ 3x storage footprint vs single fused embedding
- âŒ 3 vector searches instead of 1 (multi-index mode)
- âŒ More complex infrastructure

---

## ğŸ”€ Fusion Methods

Three methods for combining multi-vector search results:

### 1. Reciprocal Rank Fusion (RRF)

**Formula:**
```
score(d) = Î£ w_m / (k + rank_m(d))

Where:
  w_m = modality weight
  k = 60 (standard RRF constant)
  rank_m(d) = rank of document d in modality m
```

**Implementation:** `search_client.py:301`

**Characteristics:**
- âœ… **Robust** to score distribution differences
- âœ… **Emphasizes agreement** between modalities
- âœ… **Standard approach** (used by Elasticsearch, etc.)
- âœ… Better for **diverse query distributions**

**Default Weights:**
```python
{
  "visual": 0.8,      # 80% weight on visual ranking
  "audio": 0.1,       # 10% weight on audio ranking
  "transcription": 0.05  # 5% weight on transcription ranking
}
```

**API Usage:**
```python
results = client.search(
    query="person running in park",
    fusion_method="rrf",
    weights={"visual": 0.8, "audio": 0.1, "transcription": 0.05}
)
```

---

### 2. Weighted Score Fusion

**Formula:**
```
score(s) = Î£ w_m Â· sim(Q_m, E_m(s))

Where:
  w_m = modality weight
  sim() = cosine similarity
  Q_m = query embedding for modality m
  E_m(s) = segment embedding for modality m
```

**Implementation:** `search_client.py:359`

**Characteristics:**
- âœ… **Direct score combination**
- âœ… **Simpler** than RRF
- âš ï¸ Sensitive to score distributions
- âœ… Works well with **normalized scores**

**Default Weights:**
```python
{
  "visual": 0.8,
  "audio": 0.1,
  "transcription": 0.1
}
```

**API Usage:**
```python
results = client.search(
    query="person running in park",
    fusion_method="weighted",
    weights={"visual": 0.8, "audio": 0.1, "transcription": 0.1}
)
```

---

### 3. Intent-Based Dynamic Routing

**Implementation:** Uses embedding similarity to anchor prompts to automatically compute weights.

**How It Works:**
1. Pre-compute anchor embeddings for each modality (at startup)
2. For each query, compute cosine similarity to each anchor
3. Apply softmax with temperature to get normalized weights

**Formula:**
```
(w_v, w_a, w_t) = softmax(Î± Â· sim(E_query, [E_AncV, E_AncA, E_AncT]))

Where:
  Î± = temperature (default: 10.0)
  E_query = query embedding
  E_AncV/A/T = anchor embeddings for visual/audio/transcription
```

**Anchor Prompts:**
```python
VISUAL_ANCHOR = "What appears on screen: people, objects, scenes, actions,
                 clothing, colors, and visual composition of the video."

AUDIO_ANCHOR = "The non-speech audio in the video: music, sound effects,
                ambient sound, and other audio elements."

TRANSCRIPTION_ANCHOR = "The spoken words in the video: dialogue, narration,
                        speech, and what people say."
```

**Implementation:** `search_client.py:136-184`

**Characteristics:**
- âœ… **Query-adaptive** - weights change per query
- âœ… **Deterministic** - same query = same weights
- âœ… **Explainable** - can inspect anchor similarities
- âœ… **No training required** - uses embedding space directly
- âœ… **Fast iteration** - update anchors without retraining

**API Usage:**
```python
response = client.search_dynamic(
    query="explosion with loud bang",
    temperature=10.0  # Higher = more uniform, lower = more decisive
)

print(f"Computed weights: {response['weights']}")
# Output: {"visual": 0.45, "audio": 0.42, "transcription": 0.13}

print(f"Anchor similarities: {response['similarities']}")
# Output: {"visual": 0.78, "audio": 0.75, "transcription": 0.45}
```

**Temperature Effects:**
| Temperature | Behavior | Example Weights (visual, audio, transcription) |
|-------------|----------|-----------------------------------------------|
| `Î± = 1.0` | Very decisive (sharp distribution) | 0.89, 0.08, 0.03 |
| `Î± = 10.0` (default) | Balanced adaptation | 0.45, 0.42, 0.13 |
| `Î± = 50.0` | Uniform (ignores differences) | 0.34, 0.33, 0.33 |

---

## ğŸ§  LLM Query Decomposition

**Purpose:** Decompose complex natural language queries into modality-specific sub-queries for enhanced precision.

**Implementation:** `bedrock_client.py:256-401`

**How It Works:**
1. User provides a natural language query
2. Claude 3 Haiku decomposes it into three distinct queries:
   - **Visual query**: What appears on screen
   - **Audio query**: Non-speech sounds only
   - **Transcription query**: Spoken words and dialogue
3. Each sub-query gets its own embedding
4. Separate vector searches per modality using appropriate embeddings

**Example:**

**Input Query:**
```
"Ross says I take thee Rachel at a wedding"
```

**LLM Decomposition:**
```python
{
  "visual": "Ross at a wedding ceremony, wedding altar, formal attire",
  "audio": "wedding music, ceremony sounds, emotional atmosphere",
  "transcription": "Ross says I take thee Rachel"
}
```

**Model Configuration:**
- **Model:** Claude 3 Haiku (`anthropic.claude-3-haiku-20240307-v1:0`)
- **Temperature:** 0.3 (low for deterministic structured output)
- **Max Tokens:** 500

**API Usage:**
```python
# Enable decomposition with flag
results = client.search(
    query="Ross says I take thee Rachel at a wedding",
    fusion_method="rrf",
    decomposed_queries=client.bedrock.decompose_query(query)
)
```

**Web UI:** Enable "Use LLM Decomposition" toggle

**Characteristics:**
- âœ… **Precision boost** for complex multi-modal queries
- âœ… **Extracts distinct signals** from ambiguous queries
- âœ… **Context-aware expansion** - infers relevant elements
- âš ï¸ **Adds latency** (~500ms for LLM call)
- âš ï¸ **Requires Bedrock access** to Claude models

**Best For:**
- Complex queries spanning multiple modalities
- Queries where visual/audio/speech elements are intertwined
- When maximum precision is more important than latency

**Not Recommended For:**
- Simple single-modality queries ("red car")
- High-throughput/low-latency requirements
- Cost-sensitive applications (adds LLM inference cost)

---

## âš–ï¸ Modality Weight Configurations

### 1. Fixed Weights

**Method:** Manually set or statistically optimized weights applied to all queries.

**Default (Visual-Heavy):**
```python
VISUAL_WEIGHT = 0.8
AUDIO_WEIGHT = 0.1
TRANSCRIPTION_WEIGHT = 0.1
```

**Recommended Configurations by Use Case:**

| Use Case | Visual | Audio | Transcription | Example Query |
|----------|--------|-------|---------------|---------------|
| **Visual-Centric** | 0.80 | 0.10 | 0.10 | "person running", "red car crash" |
| **Dialogue-Focused** | 0.20 | 0.10 | 0.70 | "what did they say about revenue", "find where he mentions the deadline" |
| **Audio Events** | 0.30 | 0.60 | 0.10 | "explosion sound", "alarm ringing", "music playing" |
| **Balanced** | 0.40 | 0.30 | 0.30 | "wedding ceremony", "basketball game" |
| **Speech-Heavy + Visual** | 0.40 | 0.10 | 0.50 | "presenter showing slides", "interview about product" |

**Configuration Methods:**

**1. Environment Variables:**
```bash
export WEIGHT_VISUAL=0.8
export WEIGHT_AUDIO=0.1
export WEIGHT_TRANSCRIPTION=0.1
```

**2. API Parameters:**
```python
results = client.search(
    query="person laughing at joke",
    weights={"visual": 0.4, "audio": 0.3, "transcription": 0.3}
)
```

**3. Web UI Sliders:**
- Adjust visual/audio/transcription sliders in real-time
- Weights automatically normalize to sum to 1.0

**Statistical Optimization (Advanced):**

If you have historical query data with ground truth relevance labels:

```python
from search_optimization import optimize_weights

# Your evaluation dataset
eval_queries = [
    {"query": "person running", "relevant_segments": [...]},
    {"query": "alarm sound", "relevant_segments": [...]},
    # ... more examples
]

# Run grid search or Bayesian optimization
optimal_weights = optimize_weights(
    eval_queries=eval_queries,
    metric="precision@10",  # or "recall@20", "map", etc.
    search_space={
        "visual": (0.1, 0.9),
        "audio": (0.05, 0.5),
        "transcription": (0.05, 0.7)
    }
)

print(optimal_weights)
# Output: {"visual": 0.72, "audio": 0.13, "transcription": 0.15}
```

**Characteristics:**
- âœ… **Simple** - no ML training required
- âœ… **Predictable** - same weights for all queries
- âœ… **Fast** - no per-query computation
- âš ï¸ **Not adaptive** - can't adjust to query intent
- âš ï¸ **Requires domain knowledge** or labeled data for optimization

---

### 2. Dynamic Routing with Anchors

**Method:** Automatically compute weights per query using anchor similarity.

See [Intent-Based Dynamic Routing](#3-intent-based-dynamic-routing) above for detailed explanation.

**Query-Specific Weight Examples:**

| Query | Visual | Audio | Transcription | Reasoning |
|-------|--------|-------|---------------|-----------|
| "person running in park" | 0.71 | 0.15 | 0.14 | Strong visual signal |
| "explosion with loud bang" | 0.45 | 0.42 | 0.13 | Visual + audio balanced |
| "he says I take thee Rachel" | 0.22 | 0.12 | 0.66 | Heavily speech-focused |
| "wedding ceremony music" | 0.38 | 0.47 | 0.15 | Audio-dominant |
| "red car crash" | 0.68 | 0.18 | 0.14 | Visual with some audio |

**API Usage:**
```python
response = client.search_dynamic(
    query="explosion with loud bang",
    temperature=10.0,
    limit=50
)

# Inspect computed weights
print(f"Query: {query}")
print(f"Visual weight: {response['weights']['visual']:.2f}")
print(f"Audio weight: {response['weights']['audio']:.2f}")
print(f"Transcription weight: {response['weights']['transcription']:.2f}")

# Results
for result in response['results']:
    print(f"Segment {result['segment_id']}: {result['fusion_score']:.3f}")
```

---

## ğŸ—ï¸ Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   S3 Bucket     â”‚     â”‚  AWS Lambda      â”‚
â”‚   (Videos)      â”‚â”€â”€â”€â”€â–¶â”‚  (Processing)    â”‚
â”‚                 â”‚     â”‚                  â”‚
â”‚ your-media-bucket-name/ â”‚     â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚ WBD_project/    â”‚     â”‚  â”‚  Bedrock   â”‚  â”‚
â”‚ Videos/Ready/   â”‚     â”‚  â”‚  Marengo   â”‚  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚  â”‚  3.0       â”‚  â”‚
         â”‚              â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
    S3 Trigger          â”‚                  â”‚
    (automatic)         â”‚  Embeddings:     â”‚
                        â”‚  - Visual (512d) â”‚
                        â”‚  - Audio (512d)  â”‚
                        â”‚  - Transcription â”‚
                        â”‚    (512d)        â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                       DUAL-WRITE                â”‚
         â”‚                                                 â”‚
         â–¼                                                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   MongoDB Atlas         â”‚               â”‚   Amazon S3 Vectors         â”‚
â”‚   (Dual Mode)           â”‚               â”‚   (Multi-Index Only)        â”‚
â”‚                         â”‚               â”‚                             â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚               â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ unified-embeddings  â”‚ â”‚               â”‚ â”‚  visual-embeddings      â”‚ â”‚
â”‚ â”‚ (single-index mode) â”‚ â”‚               â”‚ â”‚                         â”‚ â”‚
â”‚ â”‚  + modality_type    â”‚ â”‚               â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚               â”‚ â”‚  audio-embeddings       â”‚ â”‚
â”‚                         â”‚               â”‚ â”‚                         â”‚ â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚               â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚
â”‚ â”‚ visual_embeddings   â”‚ â”‚               â”‚ â”‚  transcription-embs     â”‚ â”‚
â”‚ â”‚ (multi-index mode)  â”‚ â”‚               â”‚ â”‚                         â”‚ â”‚
â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚               â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚ â”‚ audio_embeddings    â”‚ â”‚               â”‚  Bucket: brice-video-       â”‚
â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚               â”‚  search-multimodal          â”‚
â”‚ â”‚ transcription_embs  â”‚ â”‚               â”‚                             â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚               â”‚  Score Fix (2026-02-05):    â”‚
â”‚                         â”‚               â”‚  score = 1 - (distance/2)   â”‚
â”‚  All with HNSW Indexes â”‚               â”‚  (squared Euclidean)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚                                          â”‚
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   CloudFront    â”‚     â”‚  AWS App Runner  â”‚
â”‚   (CDN)         â”‚â—€â”€â”€â”€â”€â”‚  (Search API)    â”‚
â”‚                 â”‚     â”‚                  â”‚
â”‚ Video streaming â”‚     â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚ + thumbnails    â”‚     â”‚  â”‚  FastAPI   â”‚  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚  â”‚  + Multi   â”‚  â”‚
                        â”‚  â”‚    Fusion  â”‚  â”‚
                        â”‚  â”‚  + Dynamic â”‚  â”‚
                        â”‚  â”‚    Routing â”‚  â”‚
                        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                        â”‚                  â”‚
                        â”‚  Fusion Methods: â”‚
                        â”‚  - RRF           â”‚
                        â”‚  - Weighted      â”‚
                        â”‚  - Dynamic       â”‚
                        â”‚                  â”‚
                        â”‚  Query Modes:    â”‚
                        â”‚  - LLM Decomp    â”‚
                        â”‚  - Single Query  â”‚
                        â”‚                  â”‚
                        â”‚  Backend Toggle: â”‚
                        â”‚  - MongoDB       â”‚
                        â”‚  - S3 Vectors    â”‚
                        â”‚                  â”‚
                        â”‚  Index Mode:     â”‚
                        â”‚  - Single-Index  â”‚
                        â”‚  - Multi-Index   â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ–¥ï¸ Search UI Features

The web interface provides comprehensive search capabilities:

### Search Modes

**Multi-Vector Fusion:**
- **RRF** - Reciprocal Rank Fusion (rank-based, most robust)
- **Weighted** - Score-based fusion with adjustable weights
- **Dynamic** - Intent-based routing with automatic weight calculation

**Single Modality:**
- **Visual** - Visual content only (scenes, actions, objects)
- **Audio** - Audio/sound only (music, sound effects, ambient)
- **Speech** - Transcription/dialogue only (spoken words)

### Query Options

- **LLM Decomposition** - Enable/disable query decomposition with Claude
- **Modality Weights** - Real-time sliders for visual/audio/transcription weights
- **Temperature Control** - Adjust softmax temperature for dynamic routing (1-50)

### Storage Backend Selection

**MongoDB Atlas:**
- Single-Index mode: Uses `unified-embeddings` collection with `modality_type` filter
- Multi-Index mode: Uses separate collections per modality (requires M10+ cluster)
- Toggle between modes in UI (requires migration + indexes)

**Amazon S3 Vectors:**
- Multi-Index mode only: Separate indexes per modality
- Single-index toggle disabled (unified index removed for performance)

**Score Calculation:**
- MongoDB: Native cosine similarity (vectorSearchScore)
- S3 Vectors: `1 - (squared_euclidean_distance / 2)` for normalized vectors

### Result Card Layout

Each search result displays comprehensive match information:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ #1           85%     [VIS]  â”‚  â† Rank, Confidence %, Dominant Modality
â”‚                             â”‚
â”‚     [Video Thumbnail]       â”‚
â”‚                             â”‚
â”‚         0:30 - 1:15         â”‚  â† Timestamp Range
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  Video Title
  vis: 0.85  aud: 0.12  tra: 0.03  â† Individual Modality Scores
  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘ â–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘ â–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  â† Visual Score Bars
```

**Key Features:**
- **Ranking Badge** (#1, #2, #3...) - Shows result position
- **Confidence %** - Match confidence (0-100%)
- **Dominant Badge** - Which modality scored highest (VIS/AUD/TRA)
- **Modality Scores** - Detailed breakdown per embedding type
- **Score Visualization** - Visual bars showing relative strengths
- **20 Results per Page** - Focused, high-quality results

---

## ğŸ“ Project Structure

```
multi-modal-video-search/
â”œâ”€â”€ app.py                           # FastAPI web application (search API)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ lambda_function.py           # Lambda handler for video processing
â”‚   â”œâ”€â”€ bedrock_client.py            # Bedrock Marengo client + LLM decomposition
â”‚   â”œâ”€â”€ mongodb_client.py            # MongoDB embedding storage (dual-write support)
â”‚   â”œâ”€â”€ s3_vectors_client.py         # S3 Vectors embedding storage & search
â”‚   â””â”€â”€ search_client.py             # Multi-vector search with all fusion methods
â”œâ”€â”€ static/
â”‚   â””â”€â”€ index.html                   # Search UI frontend (responsive design)
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ deploy.sh                    # AWS CLI deployment script
â”‚   â””â”€â”€ mongodb_setup.md             # MongoDB Atlas setup guide
â”œâ”€â”€ requirements.txt                 # Python dependencies
â”œâ”€â”€ .env.example                     # Environment variables template
â””â”€â”€ README.md                        # This file
```

---

---

## ğŸš€ Quick Start

### 1. Clone and Setup

```bash
cd multi-modal-video-search

# Create virtual environment
python3 -m venv venv
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt

# Copy and configure environment
cp .env.example .env
# Edit .env with your MongoDB URI and other settings
```

### 2. Setup MongoDB Atlas

Follow the detailed guide in [scripts/mongodb_setup.md](scripts/mongodb_setup.md):

1. Create a cluster:
   - **For single-index mode only:** M0 Free tier works
   - **For both single + multi-index:** M10+ required (supports 4 vector indexes)
2. Create database user and get connection string
3. Create the `unified-embeddings` collection with vector index
4. Whitelist IPs (or use 0.0.0.0/0 for testing)
5. Update `MONGODB_URI` in your `.env` file

**Optional:** Setup multi-index mode (requires M10+ cluster - see below for index creation)

### 3. Setup S3 Vectors (Alternative to MongoDB)

Amazon S3 Vectors provides a serverless vector storage option with automatic scaling and no infrastructure management.

**Create S3 Bucket:**
```bash
aws s3 mb s3://your-bucket-name --region us-east-1
```

**Create Vector Indexes:**
```bash
# Visual embeddings index
aws s3-vectors create-vector-index \
  --bucket-name your-bucket-name \
  --index-name visual-embeddings \
  --embedding-dimension 512 \
  --distance-metric COSINE \
  --region us-east-1

# Audio embeddings index
aws s3-vectors create-vector-index \
  --bucket-name your-bucket-name \
  --index-name audio-embeddings \
  --embedding-dimension 512 \
  --distance-metric COSINE \
  --region us-east-1

# Transcription embeddings index
aws s3-vectors create-vector-index \
  --bucket-name your-bucket-name \
  --index-name transcription-embeddings \
  --embedding-dimension 512 \
  --distance-metric COSINE \
  --region us-east-1
```

**Update Configuration:**
```bash
# In .env file
S3_VECTORS_BUCKET=your-bucket-name
AWS_REGION=us-east-1
```

**IAM Permissions Required:**
```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "s3-vectors:PutVector",
        "s3-vectors:QueryVectors",
        "s3-vectors:DeleteVector"
      ],
      "Resource": "arn:aws:s3-vectors:us-east-1:*:bucket/your-bucket-name/*"
    }
  ]
}
```

**Note:** S3 Vectors only supports multi-index mode in this implementation. The single-index (unified) mode has been removed for performance reasons.

---

### ğŸ”§ Bring Your Own Vector Storage

This project's vector storage layer is abstracted and can be easily replaced with your preferred backend:

**Supported Out-of-the-Box:**
- MongoDB Atlas (single + multi-index modes)
- Amazon S3 Vectors (multi-index mode)

**Easy to Integrate:**
To use a different vector database (Pinecone, Weaviate, Qdrant, Milvus, etc.):

1. Create a new client class in `src/` following the interface pattern:
   ```python
   class YourVectorClient:
       def store_segment_embeddings(self, video_id, segment_id, embeddings, ...):
           # Store embeddings in your vector DB
           pass

       def vector_search(self, query_embedding, limit, modality_filter=None):
           # Search your vector DB
           pass
   ```

2. Update `src/search_client.py` to use your client
3. Update `src/lambda_function.py` to write to your storage

**Key Requirements:**
- Support 512-dimensional embeddings (Bedrock Marengo 3.0)
- Cosine similarity distance metric
- Metadata filtering by `modality_type`, `video_id`, `segment_id`
- Return results with similarity scores (0-1 range)

The architecture is designed to be storage-agnostic - feel free to modify the code to fit your infrastructure.

---

### 4. Deploy Lambda Function

```bash
# Set MongoDB URI
export MONGODB_URI="your_mongodb_connection_string_here"

# Deploy
./scripts/deploy.sh
```

### 5. Run Search API Locally

```bash
# Start the FastAPI server
python app.py

# Open browser to http://localhost:8000
```

### 6. Process a Video

```bash
# Invoke Lambda
aws lambda invoke \
  --function-name video-embedding-pipeline \
  --region us-east-1 \
  --payload '{"s3_key": "WBD_project/Videos/Ready/sample.mp4", "bucket": "your-media-bucket-name"}' \
  --cli-binary-format raw-in-base64-out \
  response.json
```

### 7. Search Videos

**Via Web UI:** http://localhost:8000

**Via API:**
```bash
# Simple search with RRF fusion
curl "http://localhost:8000/api/search" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "person running in park",
    "fusion_method": "rrf",
    "limit": 10
  }'

# Dynamic routing search
curl "http://localhost:8000/api/search/dynamic" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "explosion with loud bang",
    "temperature": 10.0,
    "limit": 10
  }'

# With LLM decomposition
curl "http://localhost:8000/api/search" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "Ross says I take thee Rachel at a wedding",
    "use_decomposition": true,
    "fusion_method": "rrf",
    "limit": 10
  }'
```

---

## ğŸ“Š MongoDB Schema

MongoDB now supports **dual storage mode** - both single-index and multi-index simultaneously.

### Collections

**Single-Index Mode:**
- `unified-embeddings` - All modalities in one collection with `modality_type` field

**Multi-Index Mode:**
- `visual_embeddings` - Visual modality only
- `audio_embeddings` - Audio modality only
- `transcription_embeddings` - Transcription modality only

### Document Schema

**unified-embeddings collection:**
```json
{
  "_id": "ObjectId",
  "video_id": "string - unique video identifier",
  "segment_id": "int - segment index within video",
  "modality_type": "string - 'visual' | 'audio' | 'transcription'",
  "s3_uri": "string - s3://bucket/key",
  "embedding": "[float] - 512-dimensional vector",
  "start_time": "float - segment start (seconds)",
  "end_time": "float - segment end (seconds)",
  "created_at": "datetime - document creation time"
}
```

**Modality-specific collections (visual_embeddings, audio_embeddings, transcription_embeddings):**
```json
{
  "_id": "ObjectId",
  "video_id": "string - unique video identifier",
  "segment_id": "int - segment index within video",
  "s3_uri": "string - s3://bucket/key",
  "embedding": "[float] - 512-dimensional vector",
  "start_time": "float - segment start (seconds)",
  "end_time": "float - segment end (seconds)",
  "created_at": "datetime - document creation time"
}
```
*Note: No `modality_type` field needed - collection name implies modality*

### Vector Index Definitions

**unified-embeddings:**
- **Index name:** `unified_embeddings_vector_index`
- **Fields:**
  - `embedding` - vector, 512 dimensions, cosine similarity
  - `modality_type` - filter field
  - `video_id` - filter field

**visual_embeddings:**
- **Index name:** `visual_embeddings_vector_index`
- **Fields:**
  - `embedding` - vector, 512 dimensions, cosine similarity
  - `video_id` - filter field

**audio_embeddings:**
- **Index name:** `audio_embeddings_vector_index`
- **Fields:**
  - `embedding` - vector, 512 dimensions, cosine similarity
  - `video_id` - filter field

**transcription_embeddings:**
- **Index name:** `transcription_embeddings_vector_index`
- **Fields:**
  - `embedding` - vector, 512 dimensions, cosine similarity
  - `video_id` - filter field

### Migrating to Multi-Index Mode

Run the migration script to copy data from `unified-embeddings` into modality-specific collections:

```bash
python migrate_mongodb_multi_index.py
```

Then create the 3 vector search indexes in MongoDB Atlas UI (see script output for details).

**Requirements:**
- MongoDB Atlas M10+ cluster (Flex tier supports max 3 indexes, you need 4 total)
- ~3x storage space (data is duplicated across collections)

---

## ğŸ§ª API Reference

### VideoSearchClient (search_client.py)

```python
from src.search_client import VideoSearchClient

client = VideoSearchClient(
    mongodb_uri="mongodb+srv://...",
    database_name="video_search"
)

# ============ RRF Fusion Search ============
results = client.search(
    query="person running",
    fusion_method="rrf",
    weights={"visual": 0.8, "audio": 0.1, "transcription": 0.1},
    limit=10
)

# ============ Weighted Fusion Search ============
results = client.search(
    query="person running",
    fusion_method="weighted",
    weights={"visual": 0.8, "audio": 0.1, "transcription": 0.1},
    limit=10
)

# ============ Dynamic Intent Routing ============
response = client.search_dynamic(
    query="explosion with loud bang",
    temperature=10.0,
    limit=10
)
print(f"Computed weights: {response['weights']}")
print(f"Anchor similarities: {response['similarities']}")

# ============ With LLM Query Decomposition ============
decomposed = client.bedrock.decompose_query("Ross says I take thee Rachel at a wedding")
print(f"Visual: {decomposed['visual']}")
print(f"Audio: {decomposed['audio']}")
print(f"Transcription: {decomposed['transcription']}")

results = client.search(
    query="Ross says I take thee Rachel at a wedding",
    decomposed_queries=decomposed,
    fusion_method="rrf",
    limit=10
)

# ============ Single Modality Search ============
results = client.search(
    query="person running",
    modalities=["visual"],  # Only search visual modality
    limit=10
)
```

### BedrockMarengoClient (bedrock_client.py)

```python
from src.bedrock_client import BedrockMarengoClient

client = BedrockMarengoClient(region="us-east-1")

# ============ Generate Video Embeddings ============
result = client.get_video_embeddings(
    bucket="your-media-bucket-name",
    s3_key="WBD_project/Videos/file.mp4",
    embedding_types=["visual", "audio", "transcription"]
)

# ============ Generate Query Embedding ============
query_result = client.get_text_query_embedding("a car driving fast")

# ============ LLM Query Decomposition ============
decomposed = client.decompose_query("Ross says I take thee Rachel at a wedding")
print(decomposed)
# {
#   "original_query": "Ross says I take thee Rachel at a wedding",
#   "visual": "Ross at a wedding ceremony, wedding altar, formal attire",
#   "audio": "wedding music, ceremony sounds, emotional atmosphere",
#   "transcription": "Ross says I take thee Rachel"
# }
```

---

## ğŸ”§ Environment Variables

| Variable | Default | Description |
|----------|---------|-------------|
| `MONGODB_URI` | Required | MongoDB connection string |
| `MONGODB_DATABASE` | `video_search` | Database name |
| `AWS_REGION` | `us-east-1` | AWS region for Bedrock |
| `S3_BUCKET` | `your-media-bucket-name` | S3 bucket for videos |
| `CLOUDFRONT_DOMAIN` | `xxxxx.cloudfront.net` | CloudFront domain |
| `WEIGHT_VISUAL` | `0.8` | Default visual weight (fixed mode) |
| `WEIGHT_AUDIO` | `0.1` | Default audio weight (fixed mode) |
| `WEIGHT_TRANSCRIPTION` | `0.1` | Default transcription weight (fixed mode) |

---

